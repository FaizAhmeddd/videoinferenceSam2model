{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xb8zjCcy5yY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install SAM2 and dependencies"
      ],
      "metadata": {
        "id": "oJh2LKPhy_-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd {HOME}/segment-anything-2\n",
        "!pip install -e . -q\n",
        "!python setup.py build_ext --inplace"
      ],
      "metadata": {
        "id": "gVVsJfkvy9uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download SAM2 checkpoints\n",
        "NOTE: SAM2 is available in 4 different model sizes ranging from the lightweight \"sam2_hiera_tiny\" (38.9M parameters) to the more powerful \"sam2_hiera_large\" (224.4M parameters)."
      ],
      "metadata": {
        "id": "0kD5XlZ3zCZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt -P {HOME}/checkpoints\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P {HOME}/checkpoints"
      ],
      "metadata": {
        "id": "NzBAWXcuzCCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from pathlib import Path\n",
        "from supervision.assets import download_assets, VideoAssets\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "IS_COLAB = True\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "from jupyter_bbox_widget import BBoxWidget"
      ],
      "metadata": {
        "id": "RiBCqhYXzN45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "aNwSZesKzR3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2_video_predictor(CONFIG, CHECKPOINT)"
      ],
      "metadata": {
        "id": "hIn-yJBZzT7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_VIDEO = download_assets(VideoAssets.BASKETBALL)\n",
        "sv.VideoInfo.from_video_path(SOURCE_VIDEO)"
      ],
      "metadata": {
        "id": "aeN1ButAzWrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCALE_FACTOR = 0.5\n",
        "START_IDX = 100\n",
        "END_IDX = 300"
      ],
      "metadata": {
        "id": "0wP7LmpjzalR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_FRAMES = Path(HOME) / Path(SOURCE_VIDEO).stem\n",
        "SOURCE_FRAMES.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "frames_generator = sv.get_video_frames_generator(SOURCE_VIDEO, start=START_IDX, end=END_IDX)\n",
        "images_sink = sv.ImageSink(\n",
        "    target_dir_path=SOURCE_FRAMES.as_posix(),\n",
        "    overwrite=True,\n",
        "    image_name_pattern=\"{:05d}.jpeg\"\n",
        ")\n",
        "\n",
        "with images_sink:\n",
        "    for frame in frames_generator:\n",
        "        frame = sv.scale_image(frame, SCALE_FACTOR)\n",
        "        images_sink.save_image(frame)\n",
        "\n",
        "TARGET_VIDEO = Path(HOME) / f\"{Path(SOURCE_VIDEO).stem}-result.mp4\"\n",
        "SOURCE_FRAME_PATHS = sorted(sv.list_files_with_extensions(SOURCE_FRAMES.as_posix(), extensions=[\"jpeg\"]))"
      ],
      "metadata": {
        "id": "OfFOwXEkzdJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_state = sam2_model.init_state(video_path=SOURCE_FRAMES.as_posix())"
      ],
      "metadata": {
        "id": "AgRtH816zfg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        image_bytes = f.read()\n",
        "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "    return \"data:image/jpg;base64,\"+encoded"
      ],
      "metadata": {
        "id": "rg0xss40zfZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FRAME_IDX = 0\n",
        "FRAME_PATH = Path(SOURCE_FRAMES) / f\"{FRAME_IDX:05d}.jpeg\"\n",
        "\n",
        "widget = BBoxWidget(classes=OBJECTS)\n",
        "widget.image = encode_image(FRAME_PATH)\n",
        "widget"
      ],
      "metadata": {
        "id": "lOUJ4_4lzfQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_box = [\n",
        "    {'x': 705, 'y': 302, 'width': 0, 'height': 0, 'label': 'ball'},\n",
        "    {'x': 587, 'y': 300, 'width': 0, 'height': 0, 'label': 'player-1'},\n",
        "    {'x': 753, 'y': 267, 'width': 0, 'height': 0, 'label': 'player-2'}\n",
        "]\n",
        "\n",
        "boxes = widget.bboxes if widget.bboxes else default_box\n",
        "\n",
        "for object_id, label in enumerate(OBJECTS, start=1):\n",
        "    boxes = [box for box in widget.bboxes if box['label'] == label]\n",
        "\n",
        "    if len(boxes) == 0:\n",
        "        continue\n",
        "\n",
        "    points = np.array([\n",
        "        [\n",
        "            box['x'],\n",
        "            box['y']\n",
        "        ] for box in boxes\n",
        "    ], dtype=np.float32)\n",
        "    labels = np.ones(len(points))\n",
        "\n",
        "    _, object_ids, mask_logits = sam2_model.add_new_points(\n",
        "        inference_state=inference_state,\n",
        "        frame_idx=FRAME_IDX,\n",
        "        obj_id=object_id,\n",
        "        points=points,\n",
        "        labels=labels,\n",
        "    )"
      ],
      "metadata": {
        "id": "bykHFOQwzk4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO)\n",
        "video_info.width = int(video_info.width * SCALE_FACTOR)\n",
        "video_info.height = int(video_info.height * SCALE_FACTOR)\n",
        "\n",
        "COLORS = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700']\n",
        "mask_annotator = sv.MaskAnnotator(\n",
        "    color=sv.ColorPalette.from_hex(COLORS),\n",
        "    color_lookup=sv.ColorLookup.CLASS)\n",
        "\n",
        "frame_sample = []\n",
        "\n",
        "with sv.VideoSink(TARGET_VIDEO.as_posix(), video_info=video_info) as sink:\n",
        "    for frame_idx, object_ids, mask_logits in sam2_model.propagate_in_video(inference_state):\n",
        "        frame_path = SOURCE_FRAME_PATHS[frame_idx]\n",
        "        frame = cv2.imread(frame_path)\n",
        "        masks = (mask_logits > 0.0).cpu().numpy()\n",
        "        masks = np.squeeze(masks).astype(bool)\n",
        "\n",
        "        detections = sv.Detections(\n",
        "            xyxy=sv.mask_to_xyxy(masks=masks),\n",
        "            mask=masks,\n",
        "            class_id=np.array(object_ids)\n",
        "        )\n",
        "\n",
        "        annotated_frame = mask_annotator.annotate(scene=frame.copy(), detections=detections)\n",
        "\n",
        "        sink.write_frame(annotated_frame)\n",
        "        if frame_idx % video_info.fps == 0:\n",
        "            frame_sample.append(annotated_frame)"
      ],
      "metadata": {
        "id": "2rpc2ZFUzmnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.plot_images_grid(\n",
        "    images=frame_sample[:4],\n",
        "    grid_size=(2, 2)\n",
        ")"
      ],
      "metadata": {
        "id": "_IcvY4KYzpSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}